# -*- coding: utf-8 -*-
"""Cancer_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/140Ts9REGUtCFl7tWNML9H7szdzRHt-Fc

# **Predicting breast cancer in patients**

# Problem Statement:
Given the details of cell nuclei taken from breast mass, predict whether or not a patient
has breast cancer using the Ensembling Techniques. Perform necessary exploratory
data analysis before building the model and evaluate the model based on performance
metrics other than model accuracy.

#Dataset Information:
The dataset consists of several predictor variables and one target variable, Diagnosis.
The target variable has values 'Benign' and 'Malignant', where 'Benign' means that the
cells are not harmful or there is no cancer and 'Malignant' means that the patient has
cancer and the cells have a harmful effect
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

pd.set_option('display.max_columns', None)

wbc = pd.read_csv("/content/cancer.csv")
wbc.head()

wbc.shape

wbc.columns

wbc.isnull().sum()

wbc = wbc.drop(columns = ['id', 'Unnamed: 32'], axis=1) # Id is not a feature, Unnmaed: 32 is full of NaN values --> dropping both the features

wbc.dtypes

wbc.describe()

# Outlier detection
iqr = wbc['area_mean'].quantile(0.75) - wbc['area_mean'].quantile(0.25)
ut = wbc['area_mean'].quantile(0.75) + 1.5 * iqr
lt = wbc['area_mean'].quantile(0.25) - 1.5 * iqr

wbc.diagnosis.value_counts() # Checking the target value count to find whether it is balenced or imbalenced dataset

# Malignant --> Cancer Benign --> No Cancer, Label Encoding
wbc.diagnosis = wbc.diagnosis.map({'M':1, 'B':0})

wbc['diagnosis'].dtypes

"""# **Random Forest**"""

# X --> Features, y --> Target
X = np.array(wbc.loc[:,wbc.columns[1:]])
y = np.array(wbc['diagnosis'])

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)

X_train.shape, X_test.shape

y_train.shape, y_test.shape

# Scaling --> Scaling is not mandatory for Randomforest
std = StandardScaler()
X_train = std.fit_transform(X_train)
X_test = std.transform(X_test)

rf = RandomForestClassifier(n_estimators= 100, max_depth = 5, max_features='sqrt') # Initialize
rf.fit(X_train, y_train)                                                           # Fit
predictions = rf.predict(X_test)                                                   # Predict

# Evaluation metrics
acc = accuracy_score(y_test, predictions)
f1 = f1_score(y_test, predictions)
auroc = roc_auc_score( y_test, predictions)
print("Accuracy: {:.2f}".format(acc))
print("F1-score: {:.2f}".format(f1))
print("AUROC score: {:.2f}".format(auroc))

# Interpret model results
from sklearn import tree
from sklearn.tree import export_graphviz
import pydot

feature_col = list(wbc.loc[:,wbc.columns[1:]])

# Viewing one tree from 100
tree = rf.estimators_[4]
export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_col, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('tree.dot')
graph.write_png('tree.png')

print('The max depth of tree is:', tree.tree_.max_depth)

# Feature importances
importances = rf.feature_importances_

imp_feature = [(feature, round(importance, 2)) for feature, importance in zip(feature_col, importances)]
imp_feature = sorted(imp_feature, key = lambda x:x[1], reverse = True)
[print('Feature: {:30} Importance: {}'.format(*pair)) for pair in imp_feature];

# Giving test value and making predictions
rf.predict(np.array([[14.2,23.4,120.03,1200.04,0.156,0.132,0.172,0.097,0.24,0.872,1.543,2.360,19.874,250.19,0.012,0.098,0.176,0.0345,0.0543,0.0198,15.26,37.234,150.01,1245.75,0.147,0.234,0.49,0.15,0.365,0.207]]))

"""## **XG Boost**"""

import warnings
warnings.filterwarnings('ignore')

import xgboost as xgb
from xgboost import XGBClassifier

data_dmatrix = xgb.DMatrix(data=X, label=y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

from sklearn.model_selection import cross_val_score
import numpy as np
for lr in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.5,0.7,1]:
  model = xgb.XGBClassifier(learning_rate = lr, n_estimators=100, verbosity = 0) # initialise the model
  print("Learning rate : ", lr, " Cross-Val score : ", np.mean(cross_val_score(model, X_train, y_train, cv=10)))

"""Cross-val score is higher for learning rate = 0.14"""

params = {
            'objective':'binary:logistic',
            'max_depth': 4,
            'alpha': 10,
            'learning_rate': 0.14,
            'n_estimators':100
        }
xgb_clf = XGBClassifier(**params)
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)

print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
print('XGBoost model f1 score: {0:0.4f}'. format(f1_score(y_test, y_pred)))
print('XGBoost model auroc score: {0:0.4f}'. format(roc_auc_score(y_test, y_pred)))

"""Result:
For this particular dataset, to predict whether the patient has cancer or not both Random forest and XG boost giving ROC AUC score of more than 95%.
"""

